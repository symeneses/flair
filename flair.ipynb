{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-income",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flair in /opt/conda/lib/python3.8/site-packages (0.8.0.post1)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from flair) (0.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.8/site-packages (from flair) (2.8.1)\n",
      "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /opt/conda/lib/python3.8/site-packages (from flair) (3.8.3)\n",
      "Requirement already satisfied: janome in /opt/conda/lib/python3.8/site-packages (from flair) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /opt/conda/lib/python3.8/site-packages (from flair) (4.60.0)\n",
      "Requirement already satisfied: mpld3==0.3 in /opt/conda/lib/python3.8/site-packages (from flair) (0.3)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in /opt/conda/lib/python3.8/site-packages (from flair) (1.2.12)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (from flair) (2021.4.4)\n",
      "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from flair) (4.6.4)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /opt/conda/lib/python3.8/site-packages (from flair) (3.4.1)\n",
      "Requirement already satisfied: sentencepiece==0.1.95 in /opt/conda/lib/python3.8/site-packages (from flair) (0.1.95)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.8/site-packages (from flair) (0.24.1)\n",
      "Requirement already satisfied: langdetect in /opt/conda/lib/python3.8/site-packages (from flair) (1.0.8)\n",
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.8/site-packages (from flair) (6.0.1)\n",
      "Requirement already satisfied: transformers>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from flair) (4.5.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.8/site-packages (from flair) (0.0.8)\n",
      "Requirement already satisfied: lxml in /opt/conda/lib/python3.8/site-packages (from flair) (4.6.3)\n",
      "Requirement already satisfied: segtok>=1.5.7 in /opt/conda/lib/python3.8/site-packages (from flair) (1.5.10)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.8/site-packages (from flair) (0.8.9)\n",
      "Requirement already satisfied: numpy<1.20.0 in /opt/conda/lib/python3.8/site-packages (from flair) (1.19.5)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from flair) (1.7.0)\n",
      "Requirement already satisfied: gdown==3.12.2 in /opt/conda/lib/python3.8/site-packages (from flair) (3.12.2)\n",
      "Requirement already satisfied: torch<=1.7.1,>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from flair) (1.7.1)\n",
      "Requirement already satisfied: bpemb>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from flair) (0.3.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from hyperopt>=0.1.1->flair) (1.15.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.8/site-packages (from hyperopt>=0.1.1->flair) (0.18.2)\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.8/site-packages (from hyperopt>=0.1.1->flair) (2.5.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from hyperopt>=0.1.1->flair) (1.6.3)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.8/site-packages (from hyperopt>=0.1.1->flair) (1.6.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.8/site-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.0.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.8/site-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.1.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.25.1 in /opt/conda/lib/python3.8/site-packages (from konoha<5.0.0,>=4.0.0->flair) (2.25.1)\n",
      "Requirement already satisfied: importlib-metadata<4.0.0,>=3.7.0 in /opt/conda/lib/python3.8/site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.10.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib>=2.2.3->flair) (8.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.21.3->flair) (2.1.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from transformers>=4.0.0->flair) (20.9)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers>=4.0.0->flair) (0.0.45)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers>=4.0.0->flair) (0.10.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers>=4.0.0->flair) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch<=1.7.1,>=1.5.0->flair) (3.7.4.3)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /opt/conda/lib/python3.8/site-packages (from networkx>=2.2->hyperopt>=0.1.1->flair) (4.4.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.25.1->konoha<5.0.0,>=4.0.0->flair) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.25.1->konoha<5.0.0,>=4.0.0->flair) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.25.1->konoha<5.0.0,>=4.0.0->flair) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.25.1->konoha<5.0.0,>=4.0.0->flair) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.4.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broadband-great",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-02 10:54:47,981 ==========File train processed ==============\n",
      "2021-05-02 10:54:47,982 The longest sentences has 92 words.\n",
      "2021-05-02 10:54:47,982 Questions: 741 and Answers: 897\n",
      "2021-05-02 10:54:47,987 Processed sentences: 9263.\n",
      "2021-05-02 10:54:48,123 ==========File test processed ==============\n",
      "2021-05-02 10:54:48,125 The longest sentences has 83 words.\n",
      "2021-05-02 10:54:48,126 Questions: 249 and Answers: 315\n",
      "2021-05-02 10:54:48,126 Processed sentences: 3108.\n",
      "2021-05-02 10:54:48,244 ==========File dev processed ==============\n",
      "2021-05-02 10:54:48,246 The longest sentences has 86 words.\n",
      "2021-05-02 10:54:48,247 Questions: 247 and Answers: 289\n",
      "2021-05-02 10:54:48,248 Processed sentences: 2936.\n",
      "2021-05-02 10:54:48,249 Reading data from /root/.flair/datasets/stackoverflow_ner\n",
      "2021-05-02 10:54:48,250 Train: /root/.flair/datasets/stackoverflow_ner/train_clean.txt\n",
      "2021-05-02 10:54:48,251 Dev: /root/.flair/datasets/stackoverflow_ner/dev_clean.txt\n",
      "2021-05-02 10:54:48,252 Test: /root/.flair/datasets/stackoverflow_ner/test_clean.txt\n",
      "Corpus: 9263 train + 2936 dev + 3108 test sentences\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import STACKOVERFLOW_NER\n",
    "\n",
    "\n",
    "import flair\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "from flair.file_utils import cached_path\n",
    "\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "log = logging.getLogger(\"flair\")\n",
    "class STACKOVERFLOW_NER(ColumnCorpus):\n",
    "    def __init__(\n",
    "            self,\n",
    "            base_path: Union[str, Path] = None,\n",
    "            tag_to_bioes: str = \"ner\",\n",
    "            in_memory: bool = True,\n",
    "            **corpusargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the STACKOVERFLOW_NER corpus. The first time you call this constructor it will automatically\n",
    "        download the dataset.\n",
    "        :param base_path: Default is None, meaning that corpus gets auto-downloaded and loaded. You can override this\n",
    "        to point to a different folder but typically this should not be necessary.\n",
    "        :param tag_to_bioes: NER by default, need not be changed, but you could also select 'pos' to predict\n",
    "        POS tags instead\n",
    "        :param in_memory: If True, keeps dataset in memory giving speedups in training.\n",
    "        :param document_as_sequence: If True, all sentences of a document are read into a single Sentence object\n",
    "        \"\"\"\n",
    "        if type(base_path) == str:\n",
    "            base_path: Path = Path(base_path)\n",
    "\n",
    "        \"\"\"\n",
    "        The Datasets are represented in the Conll format.\n",
    "           In this format each line of the Dataset is in the following format:\n",
    "           <word>+\"\\t\"+<NE>\"\\t\"+<word>+\"\\t\"<markdown>\n",
    "           The end of sentence is marked with an empty line.\n",
    "           In each line NE represented the human annotated named entity \n",
    "           and <markdown> represented the code tags provided by the users who wrote the posts.\n",
    "           \"\"\"\n",
    "        # column format\n",
    "        columns = {0: \"word\", 1: \"ner\"}\n",
    "\n",
    "        # this dataset name\n",
    "        dataset_name = self.__class__.__name__.lower()\n",
    "\n",
    "        # default dataset folder is the cache root\n",
    "        if not base_path:\n",
    "            base_path = Path(flair.cache_root) / \"datasets\"\n",
    "        data_folder = base_path / dataset_name\n",
    "\n",
    "        # download data if necessary\n",
    "        STACKOVERFLOW_NER_path = \"https://raw.githubusercontent.com/jeniyat/StackOverflowNER/master/resources/annotated_ner_data/StackOverflow/\"\n",
    "\n",
    "        # data validation\n",
    "        ban_sentences = [\"code omitted for annotation\",\n",
    "                           \"omitted for annotation\",\n",
    "                           \"CODE_BLOCK :\",\n",
    "                           \"OP_BLOCK :\",\n",
    "                           \"Question_URL :\",\n",
    "                           \"Question_ID :\"\n",
    "                           ]\n",
    "        ban_tags = [\"Error_Name\",\n",
    "                \"Keyboard_IP\",\n",
    "                \"Value\",\n",
    "                \"Output_Block\"]\n",
    "        entity_mapping = {\"Library_Function\": \"Function\",\n",
    "                \"Function_Name\": \"Function\",\n",
    "                \"Class_Name\": \"Class\",\n",
    "                \"Library_Class\": \"Class\",\n",
    "                \"Organization\": \"Website\",\n",
    "                \"Library_Variable\": \"Variable\",\n",
    "                \"Variable_Name\": \"Variable\"}\n",
    "                \n",
    "        files = [\"train\", \"test\", \"dev\"]\n",
    "\n",
    "        for file in files:\n",
    "            questions = 0\n",
    "            answers = 0\n",
    "            sentences = 0\n",
    "            max_length = 0\n",
    "            words = []\n",
    "            tags = []\n",
    "            lines_sentence = []\n",
    "\n",
    "            cached_path(f\"{STACKOVERFLOW_NER_path}{file}.txt\", Path(\"datasets\") / dataset_name)\n",
    "            write_file = open(data_folder/ (file + \"_clean.txt\"), mode=\"w+\")\n",
    "            for line in open(data_folder/ (file + \".txt\"), mode=\"r\", encoding=\"utf-8\"):\n",
    "                if line.startswith(\"Question_ID\"):\n",
    "                    questions += 1\n",
    "\n",
    "                if line.startswith(\"Answer_to_Question_ID\"):\n",
    "                    answers += 1\n",
    "\n",
    "                line_values = line.strip().split()\n",
    "                if len(line_values) < 2:\n",
    "                    text = \" \".join(w for w in words)\n",
    "                    allowed = all([d not in text for d in ban_sentences])\n",
    "                    if allowed and len(text) > 0:\n",
    "                        sentences += 1\n",
    "                        max_length = max(len(words), max_length)\n",
    "                        for w, t in zip(words, tags):\n",
    "                            write_file.write(w + \"\\t\" + t + \"\\n\")\n",
    "                    write_file.write(\"\\n\")\n",
    "                    words = []\n",
    "                    tags = []\n",
    "                    lines_sentence = []\n",
    "                    continue\n",
    "                words.append(line_values[0])\n",
    "                ent_type = line_values[1]\n",
    "                label_split = ent_type.split(\"-\", 1)\n",
    "                if len(label_split) > 1:\n",
    "                    # ent_iob = label_split[0]\n",
    "                    if label_split[1] in ban_tags:\n",
    "                         ent_type = \"O\"\n",
    "                tags.append(ent_type)\n",
    "                lines_sentence.append(line)\n",
    "            write_file.close()\n",
    "\n",
    "            log.info(f\"==========File {file} processed ==============\")\n",
    "            log.info(f\"The longest sentences has {max_length} words.\")\n",
    "            log.info(f\"Questions: {questions} and Answers: {answers}\")\n",
    "            log.info(f\"Processed sentences: {sentences}.\")\n",
    "\n",
    "        super(STACKOVERFLOW_NER, self).__init__(\n",
    "            data_folder,\n",
    "            columns,\n",
    "            train_file=\"train_clean.txt\",\n",
    "            test_file=\"test_clean.txt\",\n",
    "            dev_file=\"dev_clean.txt\",\n",
    "            tag_to_bioes=tag_to_bioes,\n",
    "            encoding=\"utf-8\",\n",
    "            in_memory=in_memory,\n",
    "            label_name_map=entity_mapping,\n",
    "            **corpusargs\n",
    "        )\n",
    "\n",
    "corpus: Corpus = STACKOVERFLOW_NER()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "broken-ceremony",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary with 82 tags: <unk>, O, S-Data_Structure, S-Application, S-Code_Block, B-Code_Block, E-Code_Block, B-Application, E-Application, S-Function, S-Data_Type, S-Language, S-Library, S-Variable, S-Device, S-User_Name, S-User_Interface_Element, S-Class, B-Function, I-Function, E-Function, I-Code_Block, S-Website, B-Library, E-Library, S-Version, B-Class, E-Class, S-File_Name, B-User_Name\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 2. make the tag dictionary from the corpus\n",
    "tag_type = 'ner'\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "listed-sequence",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-510c6f91c302>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# FlairEmbeddings('news-backward'),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# PooledFlairEmbeddings('news-backward'),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mTransformerWordEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m ]\n\u001b[1;32m     18\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mStackedEmbeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStackedEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/flair/embeddings/token.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, layers, subtoken_pooling, layer_mean, fine_tune, allow_long_sentences, use_context, memory_effective_training, respect_document_boundaries, context_dropout, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;31m# load tokenizer and transformer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPreTrainedTokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m'config'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_from_auto\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0muse_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_fast\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \"\"\"\n\u001b[1;32m    397\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_from_auto\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             resolved_config_file = cached_path(\n\u001b[0m\u001b[1;32m    459\u001b[0m                 \u001b[0mconfig_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m   1166\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1426\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{url} not found in cache or force_download set to True, downloading to {temp_file.name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1428\u001b[0;31m             \u001b[0mhttp_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"storing {url} in cache at {cache_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0mcontent_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresume_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_length\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcontent_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m     progress = tqdm(\n\u001b[0m\u001b[1;32m   1280\u001b[0m         \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0munit_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munit_scale\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_printer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mncols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Prepare IPython progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mIProgress\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# #187 #451 #558 #872\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0;34m\"IProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# 3. initialize embeddings\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, ELMoEmbeddings, StackedEmbeddings, PooledFlairEmbeddings, TransformerWordEmbeddings\n",
    "\n",
    "embedding_types = [\n",
    "\n",
    "    # WordEmbeddings('glove'),\n",
    "    # ELMoEmbeddings(),\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use flair embeddings\n",
    "    # FlairEmbeddings('news-forward'),\n",
    "    # FlairEmbeddings('news-backward'),\n",
    "    # PooledFlairEmbeddings('news-backward'),\n",
    "    TransformerWordEmbeddings('bert-base-cased')\n",
    "]\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-liabilities",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "# 4. initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=64,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True,\n",
    "                                        dropout=0.1,\n",
    "                                        word_dropout=0,\n",
    "                                        locked_dropout=0,\n",
    "                                        train_initial_hidden_state=True,\n",
    "                                        use_rnn=True,\n",
    "                                        rnn_layers=1)\n",
    "\n",
    "# %%\n",
    "# 5. initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "from torch.optim.adam import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus, optimizer=Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-soundtrack",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4454397707459274e-07]\n",
      "[1.7378008287493754e-07]\n",
      "[2.0892961308540395e-07]\n",
      "[2.51188643150958e-07]\n",
      "[3.019951720402016e-07]\n",
      "[3.6307805477010137e-07]\n",
      "[4.36515832240166e-07]\n",
      "[5.248074602497725e-07]\n",
      "[6.309573444801933e-07]\n",
      "[7.585775750291837e-07]\n",
      "[9.120108393559096e-07]\n",
      "[1.096478196143185e-06]\n",
      "[1.3182567385564074e-06]\n",
      "[1.5848931924611132e-06]\n",
      "[1.9054607179632473e-06]\n",
      "[2.2908676527677735e-06]\n",
      "[2.754228703338166e-06]\n",
      "[3.311311214825911e-06]\n",
      "[3.981071705534973e-06]\n",
      "[4.7863009232263826e-06]\n",
      "[5.754399373371569e-06]\n",
      "[6.918309709189365e-06]\n",
      "[8.317637711026708e-06]\n",
      "[9.999999999999999e-06]\n",
      "[1.202264434617413e-05]\n",
      "[1.4454397707459279e-05]\n",
      "[1.737800828749376e-05]\n",
      "[2.0892961308540385e-05]\n",
      "[2.5118864315095795e-05]\n",
      "[3.019951720402016e-05]\n",
      "[3.630780547701014e-05]\n",
      "[4.365158322401661e-05]\n",
      "[5.248074602497728e-05]\n",
      "[6.309573444801929e-05]\n",
      "[7.585775750291836e-05]\n",
      "[9.120108393559096e-05]\n",
      "[0.00010964781961431851]\n",
      "[0.00013182567385564074]\n",
      "[0.0001584893192461114]\n",
      "[0.00019054607179632462]\n",
      "[0.00022908676527677726]\n",
      "[0.0002754228703338166]\n",
      "[0.0003311311214825911]\n",
      "[0.0003981071705534973]\n",
      "[0.0004786300923226385]\n",
      "[0.0005754399373371565]\n",
      "[0.0006918309709189362]\n",
      "[0.0008317637711026709]\n",
      "[0.001]\n",
      "[0.001202264434617413]\n",
      "[0.001445439770745928]\n",
      "[0.001737800828749376]\n",
      "[0.0020892961308540407]\n",
      "[0.002511886431509582]\n",
      "[0.0030199517204020187]\n",
      "[0.00363078054770101]\n",
      "[0.004365158322401656]\n",
      "[0.005248074602497722]\n",
      "[0.006309573444801929]\n",
      "[0.007585775750291836]\n",
      "[0.009120108393559097]\n",
      "[0.01096478196143185]\n",
      "[0.013182567385564075]\n",
      "[0.01584893192461114]\n",
      "[0.019054607179632484]\n",
      "[0.022908676527677745]\n",
      "[0.027542287033381692]\n",
      "[0.03311311214825908]\n",
      "[0.03981071705534969]\n",
      "[0.0478630092322638]\n",
      "[0.05754399373371566]\n",
      "[0.06918309709189363]\n",
      "[0.08317637711026708]\n",
      "[0.09999999999999999]\n",
      "[0.12022644346174131]\n",
      "[0.1445439770745928]\n",
      "[0.17378008287493762]\n",
      "[0.2089296130854041]\n",
      "[0.25118864315095824]\n",
      "[0.3019951720402019]\n",
      "[0.36307805477010097]\n",
      "[0.43651583224016566]\n",
      "[0.5248074602497723]\n",
      "[0.630957344480193]\n",
      "[0.7585775750291835]\n",
      "[0.9120108393559095]\n",
      "[1.096478196143185]\n",
      "[1.3182567385564072]\n",
      "[1.584893192461114]\n",
      "[1.9054607179632481]\n",
      "[2.290867652767775]\n",
      "[2.754228703338169]\n",
      "[3.3113112148259076]\n",
      "[3.981071705534969]\n",
      "[4.7863009232263805]\n",
      "[5.754399373371567]\n",
      "[6.918309709189362]\n",
      "2021-04-28 08:43:03,679 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:43:03,679 loss diverged - stopping early!\n",
      "2021-04-28 08:43:03,683 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:43:03,684 learning rate finder finished - plot resources/taggers/soft_ner/learning_rate.tsv\n",
      "2021-04-28 08:43:03,684 ----------------------------------------------------------------------------------------------------\n",
      "Learning_rate plots are saved in resources/taggers/soft_ner/learning_rate.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 08:43:04,695 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:43:04,696 Testing using best model ...\n",
      "2021-04-28 08:43:04,697 loading file resources/taggers/soft_ner/best-model.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.35 GiB (GPU 0; 15.90 GiB total capacity; 12.65 GiB already allocated; 409.50 MiB free; 14.84 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-92b256ece14d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 7. Eval training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m trainer.final_test('resources/taggers/soft_ner',\n\u001b[0m\u001b[1;32m     13\u001b[0m                     eval_mini_batch_size=512)\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/flair/trainers/trainer.py\u001b[0m in \u001b[0;36mfinal_test\u001b[0;34m(self, base_path, eval_mini_batch_size, num_workers)\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"best-model.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         test_results, test_loss = self.model.evaluate(\n\u001b[0m\u001b[1;32m    669\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_mini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/flair/models/sequence_tagger_model.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, sentences, out_path, embedding_storage_mode, mini_batch_size, num_workers, wsd_evaluation)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;31m# if span F1 needs to be used, use separate eval method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requires_span_F1_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwsd_evaluation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate_with_span_F1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_storage_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;31m# else, use scikit-learn to evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/flair/models/sequence_tagger_model.py\u001b[0m in \u001b[0;36m_evaluate_with_span_F1\u001b[0;34m(self, data_loader, embedding_storage_mode, mini_batch_size, out_path)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# predict for batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             loss = self.predict(batch,\n\u001b[0m\u001b[1;32m    429\u001b[0m                                 \u001b[0membedding_storage_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_storage_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                 \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/flair/models/sequence_tagger_model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentences, mini_batch_size, all_tag_prob, verbose, label_name, return_loss, embedding_storage_mode)\u001b[0m\n\u001b[1;32m    377\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreturn_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/flair/models/sequence_tagger_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/flair/embeddings/token.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, sentences, static_embeddings)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/flair/embeddings/base.py\u001b[0m in \u001b[0;36membed\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0meverything_embedded\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_embeddings_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/flair/embeddings/token.py\u001b[0m in \u001b[0;36m_add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# get hidden states from language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             all_hidden_states_in_lm = self.lm.get_representation(\n\u001b[0m\u001b[1;32m    625\u001b[0m                 \u001b[0mtext_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_marker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_marker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchars_per_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/flair/models/language_model.py\u001b[0m in \u001b[0;36mget_representation\u001b[0;34m(self, strings, start_marker, end_marker, chars_per_chunk)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0moutput_parts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/flair/models/language_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, ordered_sequence_lengths)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    582\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.35 GiB (GPU 0; 15.90 GiB total capacity; 12.65 GiB already allocated; 409.50 MiB free; 14.84 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "trainer.find_learning_rate('resources/taggers/soft_ner')\n",
    "# %%\n",
    "from flair.visual.training_curves import Plotter\n",
    "\n",
    "plotter = Plotter()\n",
    "plotter.plot_learning_rate(\"resources/taggers/soft_ner/learning_rate.tsv\")\n",
    "\n",
    "# %%\n",
    "# 7. Eval training\n",
    "\n",
    "trainer.final_test('resources/taggers/soft_ner',\n",
    "                    eval_mini_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-filing",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-28 08:43:50,140 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:43:50,142 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings('glove')\n",
      "    (list_embedding_1): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.05, inplace=False)\n",
      "        (encoder): Embedding(300, 100)\n",
      "        (rnn): LSTM(100, 2048)\n",
      "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (list_embedding_2): FlairEmbeddings(\n",
      "      (lm): LanguageModel(\n",
      "        (drop): Dropout(p=0.05, inplace=False)\n",
      "        (encoder): Embedding(300, 100)\n",
      "        (rnn): LSTM(100, 2048)\n",
      "        (decoder): Linear(in_features=2048, out_features=300, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (embedding2nn): Linear(in_features=4196, out_features=4196, bias=True)\n",
      "  (linear): Linear(in_features=4196, out_features=82, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2021-04-28 08:43:50,143 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:43:50,143 Corpus: \"Corpus: 9263 train + 2936 dev + 3108 test sentences\"\n",
      "2021-04-28 08:43:50,144 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:43:50,144 Parameters:\n",
      "2021-04-28 08:43:50,145  - learning_rate: \"0.5\"\n",
      "2021-04-28 08:43:50,146  - mini_batch_size: \"64\"\n",
      "2021-04-28 08:43:50,146  - patience: \"1\"\n",
      "2021-04-28 08:43:50,147  - anneal_factor: \"0.5\"\n",
      "2021-04-28 08:43:50,147  - max_epochs: \"20\"\n",
      "2021-04-28 08:43:50,148  - shuffle: \"True\"\n",
      "2021-04-28 08:43:50,149  - train_with_dev: \"False\"\n",
      "2021-04-28 08:43:50,149  - batch_growth_annealing: \"True\"\n",
      "2021-04-28 08:43:50,150 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:43:50,150 Model training base path: \"resources/taggers/soft_ner\"\n",
      "2021-04-28 08:43:50,151 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:43:50,152 Device: cuda:0\n",
      "2021-04-28 08:43:50,152 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:43:50,153 Embeddings storage mode: cpu\n",
      "2021-04-28 08:43:50,159 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:44:01,583 epoch 1 - iter 14/145 - loss 61187.81199997 - samples/sec: 78.45 - lr: 0.500000\n",
      "2021-04-28 08:44:13,337 epoch 1 - iter 28/145 - loss 53434.39848394 - samples/sec: 76.24 - lr: 0.500000\n",
      "2021-04-28 08:44:25,011 epoch 1 - iter 42/145 - loss 40972.31511188 - samples/sec: 76.77 - lr: 0.500000\n",
      "2021-04-28 08:44:36,712 epoch 1 - iter 56/145 - loss 32840.69197443 - samples/sec: 76.59 - lr: 0.500000\n",
      "2021-04-28 08:44:48,525 epoch 1 - iter 70/145 - loss 29128.59585772 - samples/sec: 75.86 - lr: 0.500000\n",
      "2021-04-28 08:45:00,626 epoch 1 - iter 84/145 - loss 25340.50661309 - samples/sec: 74.06 - lr: 0.500000\n",
      "2021-04-28 08:45:13,727 epoch 1 - iter 98/145 - loss 22679.29155953 - samples/sec: 68.40 - lr: 0.500000\n",
      "2021-04-28 08:45:24,656 epoch 1 - iter 112/145 - loss 20744.75475927 - samples/sec: 82.00 - lr: 0.500000\n",
      "2021-04-28 08:45:36,400 epoch 1 - iter 126/145 - loss 18964.50672128 - samples/sec: 76.31 - lr: 0.500000\n",
      "2021-04-28 08:45:47,702 epoch 1 - iter 140/145 - loss 17884.30311249 - samples/sec: 79.29 - lr: 0.500000\n",
      "2021-04-28 08:45:51,686 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:45:51,687 EPOCH 1 done: loss 17442.0059 - lr 0.5000000\n",
      "2021-04-28 08:46:54,716 DEV : loss 11950.23828125 - score 0.0155\n",
      "2021-04-28 08:47:25,437 TEST : loss 11915.060546875 - score 0.0158\n",
      "2021-04-28 08:47:25,657 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2021-04-28 08:47:29,236 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:47:35,985 epoch 2 - iter 14/145 - loss 12776.37461635 - samples/sec: 132.83 - lr: 0.500000\n",
      "2021-04-28 08:47:42,278 epoch 2 - iter 28/145 - loss 8577.10603551 - samples/sec: 142.42 - lr: 0.500000\n",
      "2021-04-28 08:47:48,512 epoch 2 - iter 42/145 - loss 7663.71608189 - samples/sec: 143.81 - lr: 0.500000\n",
      "2021-04-28 08:47:54,566 epoch 2 - iter 56/145 - loss 7262.16435896 - samples/sec: 148.06 - lr: 0.500000\n",
      "2021-04-28 08:48:00,712 epoch 2 - iter 70/145 - loss 7960.22480643 - samples/sec: 145.86 - lr: 0.500000\n",
      "2021-04-28 08:48:07,351 epoch 2 - iter 84/145 - loss 8690.34915742 - samples/sec: 135.01 - lr: 0.500000\n",
      "2021-04-28 08:48:13,217 epoch 2 - iter 98/145 - loss 8157.80370222 - samples/sec: 152.82 - lr: 0.500000\n",
      "2021-04-28 08:48:19,044 epoch 2 - iter 112/145 - loss 7732.16468920 - samples/sec: 153.82 - lr: 0.500000\n",
      "2021-04-28 08:48:25,010 epoch 2 - iter 126/145 - loss 7841.95743718 - samples/sec: 150.25 - lr: 0.500000\n",
      "2021-04-28 08:48:31,179 epoch 2 - iter 140/145 - loss 8237.87267107 - samples/sec: 145.29 - lr: 0.500000\n",
      "2021-04-28 08:48:33,230 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:48:33,231 EPOCH 2 done: loss 8137.3671 - lr 0.5000000\n",
      "2021-04-28 08:49:14,704 DEV : loss 2764.128173828125 - score 0.0383\n",
      "2021-04-28 08:49:25,105 TEST : loss 2646.087646484375 - score 0.0384\n",
      "2021-04-28 08:49:25,327 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2021-04-28 08:49:28,884 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:49:35,192 epoch 3 - iter 14/145 - loss 7658.18195452 - samples/sec: 142.12 - lr: 0.500000\n",
      "2021-04-28 08:49:41,521 epoch 3 - iter 28/145 - loss 6734.29702323 - samples/sec: 141.62 - lr: 0.500000\n",
      "2021-04-28 08:49:47,569 epoch 3 - iter 42/145 - loss 6483.79644485 - samples/sec: 148.21 - lr: 0.500000\n",
      "2021-04-28 08:49:53,993 epoch 3 - iter 56/145 - loss 6632.01147461 - samples/sec: 139.54 - lr: 0.500000\n",
      "2021-04-28 08:49:59,555 epoch 3 - iter 70/145 - loss 6689.49806955 - samples/sec: 161.16 - lr: 0.500000\n",
      "2021-04-28 08:50:06,301 epoch 3 - iter 84/145 - loss 6914.24113392 - samples/sec: 132.87 - lr: 0.500000\n",
      "2021-04-28 08:50:12,486 epoch 3 - iter 98/145 - loss 6784.65318703 - samples/sec: 144.92 - lr: 0.500000\n",
      "2021-04-28 08:50:18,812 epoch 3 - iter 112/145 - loss 6199.80864825 - samples/sec: 141.68 - lr: 0.500000\n",
      "2021-04-28 08:50:25,128 epoch 3 - iter 126/145 - loss 6418.30021353 - samples/sec: 141.92 - lr: 0.500000\n",
      "2021-04-28 08:50:31,256 epoch 3 - iter 140/145 - loss 6535.78243059 - samples/sec: 146.29 - lr: 0.500000\n",
      "2021-04-28 08:50:33,325 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:50:33,326 EPOCH 3 done: loss 6491.0148 - lr 0.5000000\n",
      "2021-04-28 08:51:15,121 DEV : loss 5713.658203125 - score 0.0006\n",
      "2021-04-28 08:51:25,382 TEST : loss 5926.84033203125 - score 0.0\n",
      "2021-04-28 08:51:25,605 BAD EPOCHS (no improvement): 1\n",
      "2021-04-28 08:51:25,609 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:51:32,043 epoch 4 - iter 14/145 - loss 4352.47007533 - samples/sec: 139.33 - lr: 0.500000\n",
      "2021-04-28 08:51:38,061 epoch 4 - iter 28/145 - loss 4856.71244594 - samples/sec: 148.97 - lr: 0.500000\n",
      "2021-04-28 08:51:44,366 epoch 4 - iter 42/145 - loss 5206.95877511 - samples/sec: 142.17 - lr: 0.500000\n",
      "2021-04-28 08:51:50,515 epoch 4 - iter 56/145 - loss 6820.18790327 - samples/sec: 145.80 - lr: 0.500000\n",
      "2021-04-28 08:51:56,888 epoch 4 - iter 70/145 - loss 6345.54481550 - samples/sec: 140.64 - lr: 0.500000\n",
      "2021-04-28 08:52:03,237 epoch 4 - iter 84/145 - loss 7167.48095122 - samples/sec: 141.20 - lr: 0.500000\n",
      "2021-04-28 08:52:09,142 epoch 4 - iter 98/145 - loss 7145.70435816 - samples/sec: 151.80 - lr: 0.500000\n",
      "2021-04-28 08:52:15,355 epoch 4 - iter 112/145 - loss 6889.37878854 - samples/sec: 144.29 - lr: 0.500000\n",
      "2021-04-28 08:52:22,070 epoch 4 - iter 126/145 - loss 6798.62875512 - samples/sec: 133.48 - lr: 0.500000\n",
      "2021-04-28 08:52:28,347 epoch 4 - iter 140/145 - loss 6822.78157785 - samples/sec: 142.80 - lr: 0.500000\n",
      "2021-04-28 08:52:30,445 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:52:30,446 EPOCH 4 done: loss 6690.4254 - lr 0.5000000\n",
      "2021-04-28 08:53:11,097 DEV : loss 2356.850830078125 - score 0.0317\n",
      "2021-04-28 08:53:21,674 TEST : loss 2355.925537109375 - score 0.0256\n",
      "Epoch     4: reducing learning rate of group 0 to 2.5000e-01.\n",
      "2021-04-28 08:53:21,892 BAD EPOCHS (no improvement): 2\n",
      "2021-04-28 08:53:21,896 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:53:21,897 resetting to best model\n",
      "2021-04-28 08:53:21,897 loading file resources/taggers/soft_ner/best-model.pt\n",
      "2021-04-28 08:53:31,497 epoch 5 - iter 7/73 - loss 2303.73244803 - samples/sec: 146.12 - lr: 0.250000\n",
      "2021-04-28 08:53:38,079 epoch 5 - iter 14/73 - loss 2219.56048148 - samples/sec: 136.20 - lr: 0.250000\n",
      "2021-04-28 08:53:44,307 epoch 5 - iter 21/73 - loss 3373.31699044 - samples/sec: 143.98 - lr: 0.250000\n",
      "2021-04-28 08:53:50,477 epoch 5 - iter 28/73 - loss 3011.88890076 - samples/sec: 145.31 - lr: 0.250000\n",
      "2021-04-28 08:53:56,780 epoch 5 - iter 35/73 - loss 2700.78853062 - samples/sec: 142.25 - lr: 0.250000\n",
      "2021-04-28 08:54:02,776 epoch 5 - iter 42/73 - loss 2480.26382737 - samples/sec: 149.53 - lr: 0.250000\n",
      "2021-04-28 08:54:09,138 epoch 5 - iter 49/73 - loss 2335.62308549 - samples/sec: 140.91 - lr: 0.250000\n",
      "2021-04-28 08:54:15,351 epoch 5 - iter 56/73 - loss 2267.53457969 - samples/sec: 144.30 - lr: 0.250000\n",
      "2021-04-28 08:54:21,422 epoch 5 - iter 63/73 - loss 2296.58772883 - samples/sec: 147.70 - lr: 0.250000\n",
      "2021-04-28 08:54:27,855 epoch 5 - iter 70/73 - loss 2221.58499146 - samples/sec: 139.35 - lr: 0.250000\n",
      "2021-04-28 08:54:29,897 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:54:29,898 EPOCH 5 done: loss 2165.6624 - lr 0.2500000\n",
      "2021-04-28 08:55:13,572 DEV : loss 3117.9755859375 - score 0.0362\n",
      "2021-04-28 08:55:24,111 TEST : loss 3059.70849609375 - score 0.042\n",
      "2021-04-28 08:55:24,334 BAD EPOCHS (no improvement): 1\n",
      "2021-04-28 08:55:24,338 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:55:30,683 epoch 6 - iter 7/73 - loss 1374.74904088 - samples/sec: 141.31 - lr: 0.250000\n",
      "2021-04-28 08:55:36,490 epoch 6 - iter 14/73 - loss 1554.07376535 - samples/sec: 154.45 - lr: 0.250000\n",
      "2021-04-28 08:55:42,268 epoch 6 - iter 21/73 - loss 1527.99971517 - samples/sec: 155.15 - lr: 0.250000\n",
      "2021-04-28 08:55:48,798 epoch 6 - iter 28/73 - loss 2238.34998431 - samples/sec: 137.31 - lr: 0.250000\n",
      "2021-04-28 08:55:55,336 epoch 6 - iter 35/73 - loss 2308.26166295 - samples/sec: 137.14 - lr: 0.250000\n",
      "2021-04-28 08:56:02,261 epoch 6 - iter 42/73 - loss 2162.94188291 - samples/sec: 129.47 - lr: 0.250000\n",
      "2021-04-28 08:56:08,406 epoch 6 - iter 49/73 - loss 2075.79031061 - samples/sec: 145.87 - lr: 0.250000\n",
      "2021-04-28 08:56:14,973 epoch 6 - iter 56/73 - loss 2086.27825928 - samples/sec: 136.52 - lr: 0.250000\n",
      "2021-04-28 08:56:20,995 epoch 6 - iter 63/73 - loss 2012.80831667 - samples/sec: 148.89 - lr: 0.250000\n",
      "2021-04-28 08:56:27,524 epoch 6 - iter 70/73 - loss 1964.41718837 - samples/sec: 137.31 - lr: 0.250000\n",
      "2021-04-28 08:56:29,605 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:56:29,606 EPOCH 6 done: loss 1950.5793 - lr 0.2500000\n",
      "2021-04-28 08:57:11,077 DEV : loss 2389.841796875 - score 0.0267\n",
      "2021-04-28 08:57:23,250 TEST : loss 2435.25927734375 - score 0.0263\n",
      "Epoch     6: reducing learning rate of group 0 to 1.2500e-01.\n",
      "2021-04-28 08:57:23,468 BAD EPOCHS (no improvement): 2\n",
      "2021-04-28 08:57:23,472 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:57:23,472 resetting to best model\n",
      "2021-04-28 08:57:23,473 loading file resources/taggers/soft_ner/best-model.pt\n",
      "2021-04-28 08:57:31,059 epoch 7 - iter 3/37 - loss 2081.07885742 - samples/sec: 140.31 - lr: 0.125000\n",
      "2021-04-28 08:57:36,058 epoch 7 - iter 6/37 - loss 1574.13531494 - samples/sec: 153.67 - lr: 0.125000\n",
      "2021-04-28 08:57:41,450 epoch 7 - iter 9/37 - loss 1348.88991292 - samples/sec: 142.52 - lr: 0.125000\n",
      "2021-04-28 08:57:46,547 epoch 7 - iter 12/37 - loss 1275.63897196 - samples/sec: 150.76 - lr: 0.125000\n",
      "2021-04-28 08:57:52,173 epoch 7 - iter 15/37 - loss 1157.90977783 - samples/sec: 136.55 - lr: 0.125000\n",
      "2021-04-28 08:57:57,272 epoch 7 - iter 18/37 - loss 1057.18468899 - samples/sec: 150.73 - lr: 0.125000\n",
      "2021-04-28 08:58:02,562 epoch 7 - iter 21/37 - loss 961.82089088 - samples/sec: 145.28 - lr: 0.125000\n",
      "2021-04-28 08:58:08,007 epoch 7 - iter 24/37 - loss 890.37222036 - samples/sec: 141.12 - lr: 0.125000\n",
      "2021-04-28 08:58:13,665 epoch 7 - iter 27/37 - loss 843.02455648 - samples/sec: 135.79 - lr: 0.125000\n",
      "2021-04-28 08:58:18,416 epoch 7 - iter 30/37 - loss 794.27021484 - samples/sec: 161.77 - lr: 0.125000\n",
      "2021-04-28 08:58:23,577 epoch 7 - iter 33/37 - loss 762.15723119 - samples/sec: 148.90 - lr: 0.125000\n",
      "2021-04-28 08:58:28,777 epoch 7 - iter 36/37 - loss 722.72907172 - samples/sec: 147.76 - lr: 0.125000\n",
      "2021-04-28 08:58:29,189 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:58:29,190 EPOCH 7 done: loss 714.4957 - lr 0.1250000\n",
      "2021-04-28 08:59:10,883 DEV : loss 459.5187683105469 - score 0.0737\n",
      "2021-04-28 08:59:21,180 TEST : loss 455.50115966796875 - score 0.0764\n",
      "2021-04-28 08:59:21,393 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2021-04-28 08:59:24,955 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 08:59:30,646 epoch 8 - iter 3/37 - loss 353.76748657 - samples/sec: 135.05 - lr: 0.125000\n",
      "2021-04-28 08:59:35,846 epoch 8 - iter 6/37 - loss 516.26598104 - samples/sec: 147.75 - lr: 0.125000\n",
      "2021-04-28 08:59:41,192 epoch 8 - iter 9/37 - loss 475.08395725 - samples/sec: 143.73 - lr: 0.125000\n",
      "2021-04-28 08:59:46,419 epoch 8 - iter 12/37 - loss 441.69315338 - samples/sec: 147.03 - lr: 0.125000\n",
      "2021-04-28 08:59:51,586 epoch 8 - iter 15/37 - loss 434.21260376 - samples/sec: 148.73 - lr: 0.125000\n",
      "2021-04-28 08:59:56,784 epoch 8 - iter 18/37 - loss 424.32705858 - samples/sec: 147.82 - lr: 0.125000\n",
      "2021-04-28 09:00:02,430 epoch 8 - iter 21/37 - loss 396.88581630 - samples/sec: 136.08 - lr: 0.125000\n",
      "2021-04-28 09:00:07,795 epoch 8 - iter 24/37 - loss 383.82602056 - samples/sec: 143.24 - lr: 0.125000\n",
      "2021-04-28 09:00:12,939 epoch 8 - iter 27/37 - loss 372.78877089 - samples/sec: 149.38 - lr: 0.125000\n",
      "2021-04-28 09:00:18,385 epoch 8 - iter 30/37 - loss 407.41260579 - samples/sec: 141.08 - lr: 0.125000\n",
      "2021-04-28 09:00:23,991 epoch 8 - iter 33/37 - loss 416.94627981 - samples/sec: 137.08 - lr: 0.125000\n",
      "2021-04-28 09:00:29,135 epoch 8 - iter 36/37 - loss 433.67440796 - samples/sec: 149.36 - lr: 0.125000\n",
      "2021-04-28 09:00:29,393 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:00:29,394 EPOCH 8 done: loss 443.7993 - lr 0.1250000\n",
      "2021-04-28 09:01:10,835 DEV : loss 474.82159423828125 - score 0.0595\n",
      "2021-04-28 09:01:23,072 TEST : loss 450.245849609375 - score 0.0608\n",
      "2021-04-28 09:01:23,307 BAD EPOCHS (no improvement): 1\n",
      "2021-04-28 09:01:23,311 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:01:28,470 epoch 9 - iter 3/37 - loss 323.46656291 - samples/sec: 148.94 - lr: 0.125000\n",
      "2021-04-28 09:01:34,278 epoch 9 - iter 6/37 - loss 403.51342265 - samples/sec: 132.28 - lr: 0.125000\n",
      "2021-04-28 09:01:39,414 epoch 9 - iter 9/37 - loss 361.80583700 - samples/sec: 149.63 - lr: 0.125000\n",
      "2021-04-28 09:01:45,068 epoch 9 - iter 12/37 - loss 429.44129181 - samples/sec: 135.93 - lr: 0.125000\n",
      "2021-04-28 09:01:50,292 epoch 9 - iter 15/37 - loss 442.35702311 - samples/sec: 147.08 - lr: 0.125000\n",
      "2021-04-28 09:01:55,598 epoch 9 - iter 18/37 - loss 446.53768921 - samples/sec: 144.81 - lr: 0.125000\n",
      "2021-04-28 09:02:00,680 epoch 9 - iter 21/37 - loss 469.87518601 - samples/sec: 151.19 - lr: 0.125000\n",
      "2021-04-28 09:02:05,447 epoch 9 - iter 24/37 - loss 489.53010559 - samples/sec: 161.24 - lr: 0.125000\n",
      "2021-04-28 09:02:10,765 epoch 9 - iter 27/37 - loss 508.47476535 - samples/sec: 144.49 - lr: 0.125000\n",
      "2021-04-28 09:02:16,079 epoch 9 - iter 30/37 - loss 538.24274089 - samples/sec: 144.62 - lr: 0.125000\n",
      "2021-04-28 09:02:21,503 epoch 9 - iter 33/37 - loss 540.90853419 - samples/sec: 141.67 - lr: 0.125000\n",
      "2021-04-28 09:02:26,794 epoch 9 - iter 36/37 - loss 535.85926988 - samples/sec: 145.27 - lr: 0.125000\n",
      "2021-04-28 09:02:27,088 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:02:27,090 EPOCH 9 done: loss 531.3218 - lr 0.1250000\n",
      "2021-04-28 09:03:07,586 DEV : loss 317.0064392089844 - score 0.0501\n",
      "2021-04-28 09:03:18,017 TEST : loss 314.2352600097656 - score 0.0468\n",
      "Epoch     9: reducing learning rate of group 0 to 6.2500e-02.\n",
      "2021-04-28 09:03:18,374 BAD EPOCHS (no improvement): 2\n",
      "2021-04-28 09:03:18,383 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:03:18,384 resetting to best model\n",
      "2021-04-28 09:03:18,385 loading file resources/taggers/soft_ner/best-model.pt\n",
      "2021-04-28 09:03:25,137 epoch 10 - iter 1/19 - loss 573.03472900 - samples/sec: 156.89 - lr: 0.062500\n",
      "2021-04-28 09:03:28,729 epoch 10 - iter 2/19 - loss 443.00317383 - samples/sec: 142.60 - lr: 0.062500\n",
      "2021-04-28 09:03:32,182 epoch 10 - iter 3/19 - loss 367.71214803 - samples/sec: 148.37 - lr: 0.062500\n",
      "2021-04-28 09:03:36,164 epoch 10 - iter 4/19 - loss 322.49954605 - samples/sec: 128.64 - lr: 0.062500\n",
      "2021-04-28 09:03:39,757 epoch 10 - iter 5/19 - loss 302.29003906 - samples/sec: 142.57 - lr: 0.062500\n",
      "2021-04-28 09:03:43,304 epoch 10 - iter 6/19 - loss 294.42893982 - samples/sec: 144.42 - lr: 0.062500\n",
      "2021-04-28 09:03:46,537 epoch 10 - iter 7/19 - loss 275.80836269 - samples/sec: 158.43 - lr: 0.062500\n",
      "2021-04-28 09:03:49,899 epoch 10 - iter 8/19 - loss 271.86388588 - samples/sec: 152.40 - lr: 0.062500\n",
      "2021-04-28 09:03:53,159 epoch 10 - iter 9/19 - loss 259.28247918 - samples/sec: 157.11 - lr: 0.062500\n",
      "2021-04-28 09:03:56,316 epoch 10 - iter 10/19 - loss 261.44259186 - samples/sec: 162.26 - lr: 0.062500\n",
      "2021-04-28 09:03:59,815 epoch 10 - iter 11/19 - loss 261.32698891 - samples/sec: 146.41 - lr: 0.062500\n",
      "2021-04-28 09:04:02,952 epoch 10 - iter 12/19 - loss 259.87981669 - samples/sec: 163.35 - lr: 0.062500\n",
      "2021-04-28 09:04:06,489 epoch 10 - iter 13/19 - loss 254.71472520 - samples/sec: 144.80 - lr: 0.062500\n",
      "2021-04-28 09:04:09,920 epoch 10 - iter 14/19 - loss 244.34089824 - samples/sec: 149.31 - lr: 0.062500\n",
      "2021-04-28 09:04:13,531 epoch 10 - iter 15/19 - loss 236.73374786 - samples/sec: 141.89 - lr: 0.062500\n",
      "2021-04-28 09:04:17,075 epoch 10 - iter 16/19 - loss 228.04693651 - samples/sec: 144.55 - lr: 0.062500\n",
      "2021-04-28 09:04:20,679 epoch 10 - iter 17/19 - loss 226.60430056 - samples/sec: 142.19 - lr: 0.062500\n",
      "2021-04-28 09:04:24,165 epoch 10 - iter 18/19 - loss 220.89088821 - samples/sec: 146.93 - lr: 0.062500\n",
      "2021-04-28 09:04:24,471 epoch 10 - iter 19/19 - loss 216.60364372 - samples/sec: 1681.75 - lr: 0.062500\n",
      "2021-04-28 09:04:24,472 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:04:24,473 EPOCH 10 done: loss 216.6036 - lr 0.0625000\n",
      "2021-04-28 09:05:05,466 DEV : loss 105.23822021484375 - score 0.1859\n",
      "2021-04-28 09:05:17,788 TEST : loss 105.33131408691406 - score 0.1952\n",
      "2021-04-28 09:05:17,998 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2021-04-28 09:05:21,554 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:05:25,163 epoch 11 - iter 1/19 - loss 79.29996490 - samples/sec: 141.97 - lr: 0.062500\n",
      "2021-04-28 09:05:28,869 epoch 11 - iter 2/19 - loss 134.99423599 - samples/sec: 138.23 - lr: 0.062500\n",
      "2021-04-28 09:05:32,145 epoch 11 - iter 3/19 - loss 123.95896912 - samples/sec: 156.40 - lr: 0.062500\n",
      "2021-04-28 09:05:35,836 epoch 11 - iter 4/19 - loss 208.06305313 - samples/sec: 138.77 - lr: 0.062500\n",
      "2021-04-28 09:05:39,481 epoch 11 - iter 5/19 - loss 284.21191711 - samples/sec: 140.59 - lr: 0.062500\n",
      "2021-04-28 09:05:42,937 epoch 11 - iter 6/19 - loss 283.30727386 - samples/sec: 148.21 - lr: 0.062500\n",
      "2021-04-28 09:05:46,545 epoch 11 - iter 7/19 - loss 272.14911979 - samples/sec: 142.02 - lr: 0.062500\n",
      "2021-04-28 09:05:50,134 epoch 11 - iter 8/19 - loss 258.46868324 - samples/sec: 142.78 - lr: 0.062500\n",
      "2021-04-28 09:05:54,092 epoch 11 - iter 9/19 - loss 255.55925157 - samples/sec: 129.41 - lr: 0.062500\n",
      "2021-04-28 09:05:57,414 epoch 11 - iter 10/19 - loss 245.71966553 - samples/sec: 154.24 - lr: 0.062500\n",
      "2021-04-28 09:06:00,964 epoch 11 - iter 11/19 - loss 237.85892278 - samples/sec: 144.29 - lr: 0.062500\n",
      "2021-04-28 09:06:04,495 epoch 11 - iter 12/19 - loss 224.71811358 - samples/sec: 145.08 - lr: 0.062500\n",
      "2021-04-28 09:06:07,604 epoch 11 - iter 13/19 - loss 215.53736467 - samples/sec: 164.80 - lr: 0.062500\n",
      "2021-04-28 09:06:10,844 epoch 11 - iter 14/19 - loss 210.20889991 - samples/sec: 158.12 - lr: 0.062500\n",
      "2021-04-28 09:06:14,386 epoch 11 - iter 15/19 - loss 205.28167572 - samples/sec: 144.62 - lr: 0.062500\n",
      "2021-04-28 09:06:17,612 epoch 11 - iter 16/19 - loss 197.24688435 - samples/sec: 158.83 - lr: 0.062500\n",
      "2021-04-28 09:06:21,420 epoch 11 - iter 17/19 - loss 189.63106582 - samples/sec: 134.50 - lr: 0.062500\n",
      "2021-04-28 09:06:25,069 epoch 11 - iter 18/19 - loss 185.29943848 - samples/sec: 140.41 - lr: 0.062500\n",
      "2021-04-28 09:06:25,449 epoch 11 - iter 19/19 - loss 181.59636528 - samples/sec: 1354.65 - lr: 0.062500\n",
      "2021-04-28 09:06:25,450 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:06:25,450 EPOCH 11 done: loss 181.5964 - lr 0.0625000\n",
      "2021-04-28 09:07:05,976 DEV : loss 94.99530029296875 - score 0.1151\n",
      "2021-04-28 09:07:16,490 TEST : loss 96.33116912841797 - score 0.095\n",
      "2021-04-28 09:07:16,707 BAD EPOCHS (no improvement): 1\n",
      "2021-04-28 09:07:16,710 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:07:20,220 epoch 12 - iter 1/19 - loss 93.95471954 - samples/sec: 146.01 - lr: 0.062500\n",
      "2021-04-28 09:07:23,470 epoch 12 - iter 2/19 - loss 99.56431961 - samples/sec: 157.61 - lr: 0.062500\n",
      "2021-04-28 09:07:27,090 epoch 12 - iter 3/19 - loss 102.75371043 - samples/sec: 141.49 - lr: 0.062500\n",
      "2021-04-28 09:07:30,902 epoch 12 - iter 4/19 - loss 121.22418022 - samples/sec: 134.41 - lr: 0.062500\n",
      "2021-04-28 09:07:34,569 epoch 12 - iter 5/19 - loss 131.18420258 - samples/sec: 139.69 - lr: 0.062500\n",
      "2021-04-28 09:07:37,820 epoch 12 - iter 6/19 - loss 132.16746902 - samples/sec: 157.57 - lr: 0.062500\n",
      "2021-04-28 09:07:41,189 epoch 12 - iter 7/19 - loss 130.52335576 - samples/sec: 152.10 - lr: 0.062500\n",
      "2021-04-28 09:07:44,872 epoch 12 - iter 8/19 - loss 145.01380062 - samples/sec: 139.09 - lr: 0.062500\n",
      "2021-04-28 09:07:48,127 epoch 12 - iter 9/19 - loss 146.91892412 - samples/sec: 157.41 - lr: 0.062500\n",
      "2021-04-28 09:07:52,171 epoch 12 - iter 10/19 - loss 145.66987839 - samples/sec: 126.64 - lr: 0.062500\n",
      "2021-04-28 09:07:55,818 epoch 12 - iter 11/19 - loss 141.36846716 - samples/sec: 140.49 - lr: 0.062500\n",
      "2021-04-28 09:07:59,316 epoch 12 - iter 12/19 - loss 139.76261775 - samples/sec: 146.45 - lr: 0.062500\n",
      "2021-04-28 09:08:02,638 epoch 12 - iter 13/19 - loss 140.54808044 - samples/sec: 154.26 - lr: 0.062500\n",
      "2021-04-28 09:08:06,087 epoch 12 - iter 14/19 - loss 142.74682944 - samples/sec: 148.55 - lr: 0.062500\n",
      "2021-04-28 09:08:09,722 epoch 12 - iter 15/19 - loss 145.64073995 - samples/sec: 140.92 - lr: 0.062500\n",
      "2021-04-28 09:08:13,143 epoch 12 - iter 16/19 - loss 146.07372665 - samples/sec: 149.80 - lr: 0.062500\n",
      "2021-04-28 09:08:16,156 epoch 12 - iter 17/19 - loss 152.02198792 - samples/sec: 169.99 - lr: 0.062500\n",
      "2021-04-28 09:08:19,568 epoch 12 - iter 18/19 - loss 151.95770518 - samples/sec: 150.17 - lr: 0.062500\n",
      "2021-04-28 09:08:19,963 epoch 12 - iter 19/19 - loss 150.63971028 - samples/sec: 1303.59 - lr: 0.062500\n",
      "2021-04-28 09:08:19,964 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:08:19,965 EPOCH 12 done: loss 150.6397 - lr 0.0625000\n",
      "2021-04-28 09:09:02,611 DEV : loss 153.39724731445312 - score 0.0965\n",
      "2021-04-28 09:09:12,836 TEST : loss 156.9483642578125 - score 0.0897\n",
      "Epoch    12: reducing learning rate of group 0 to 3.1250e-02.\n",
      "2021-04-28 09:09:13,054 BAD EPOCHS (no improvement): 2\n",
      "2021-04-28 09:09:13,058 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:09:13,059 resetting to best model\n",
      "2021-04-28 09:09:13,060 loading file resources/taggers/soft_ner/best-model.pt\n",
      "2021-04-28 09:09:23,697 epoch 13 - iter 1/10 - loss 69.77403259 - samples/sec: 143.48 - lr: 0.031250\n",
      "2021-04-28 09:09:30,454 epoch 13 - iter 2/10 - loss 80.65587234 - samples/sec: 151.61 - lr: 0.031250\n",
      "2021-04-28 09:09:37,618 epoch 13 - iter 3/10 - loss 76.95377858 - samples/sec: 142.98 - lr: 0.031250\n",
      "2021-04-28 09:09:44,622 epoch 13 - iter 4/10 - loss 79.23509789 - samples/sec: 146.26 - lr: 0.031250\n",
      "2021-04-28 09:09:51,452 epoch 13 - iter 5/10 - loss 83.37308807 - samples/sec: 149.98 - lr: 0.031250\n",
      "2021-04-28 09:09:58,339 epoch 13 - iter 6/10 - loss 80.25808080 - samples/sec: 148.73 - lr: 0.031250\n",
      "2021-04-28 09:10:05,821 epoch 13 - iter 7/10 - loss 79.32586997 - samples/sec: 136.91 - lr: 0.031250\n",
      "2021-04-28 09:10:12,612 epoch 13 - iter 8/10 - loss 80.10709953 - samples/sec: 150.86 - lr: 0.031250\n",
      "2021-04-28 09:10:19,990 epoch 13 - iter 9/10 - loss 77.31029680 - samples/sec: 138.84 - lr: 0.031250\n",
      "2021-04-28 09:10:20,481 epoch 13 - iter 10/10 - loss 75.70503578 - samples/sec: 2093.30 - lr: 0.031250\n",
      "2021-04-28 09:10:20,483 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:10:20,483 EPOCH 13 done: loss 75.7050 - lr 0.0312500\n",
      "2021-04-28 09:11:01,327 DEV : loss 58.85588836669922 - score 0.2042\n",
      "2021-04-28 09:11:11,689 TEST : loss 60.85179138183594 - score 0.2098\n",
      "2021-04-28 09:11:11,909 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2021-04-28 09:11:15,429 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:11:22,351 epoch 14 - iter 1/10 - loss 39.97546768 - samples/sec: 148.02 - lr: 0.031250\n",
      "2021-04-28 09:11:29,103 epoch 14 - iter 2/10 - loss 51.43980789 - samples/sec: 151.69 - lr: 0.031250\n",
      "2021-04-28 09:11:36,319 epoch 14 - iter 3/10 - loss 53.40876516 - samples/sec: 141.97 - lr: 0.031250\n",
      "2021-04-28 09:11:43,178 epoch 14 - iter 4/10 - loss 49.02408981 - samples/sec: 149.33 - lr: 0.031250\n",
      "2021-04-28 09:11:51,364 epoch 14 - iter 5/10 - loss 50.36809235 - samples/sec: 125.12 - lr: 0.031250\n",
      "2021-04-28 09:11:58,352 epoch 14 - iter 6/10 - loss 52.00855128 - samples/sec: 146.59 - lr: 0.031250\n",
      "2021-04-28 09:12:05,347 epoch 14 - iter 7/10 - loss 52.35552161 - samples/sec: 146.45 - lr: 0.031250\n",
      "2021-04-28 09:12:12,284 epoch 14 - iter 8/10 - loss 50.67078304 - samples/sec: 147.68 - lr: 0.031250\n",
      "2021-04-28 09:12:18,850 epoch 14 - iter 9/10 - loss 51.11550776 - samples/sec: 156.00 - lr: 0.031250\n",
      "2021-04-28 09:12:19,137 epoch 14 - iter 10/10 - loss 48.87215919 - samples/sec: 3586.95 - lr: 0.031250\n",
      "2021-04-28 09:12:19,138 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:12:19,139 EPOCH 14 done: loss 48.8722 - lr 0.0312500\n",
      "2021-04-28 09:13:01,511 DEV : loss 47.7477912902832 - score 0.1551\n",
      "2021-04-28 09:13:12,100 TEST : loss 45.00482177734375 - score 0.1738\n",
      "2021-04-28 09:13:12,317 BAD EPOCHS (no improvement): 1\n",
      "2021-04-28 09:13:12,320 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:13:19,475 epoch 15 - iter 1/10 - loss 58.47145081 - samples/sec: 143.15 - lr: 0.031250\n",
      "2021-04-28 09:13:26,825 epoch 15 - iter 2/10 - loss 56.04192352 - samples/sec: 139.37 - lr: 0.031250\n",
      "2021-04-28 09:13:33,395 epoch 15 - iter 3/10 - loss 46.17587407 - samples/sec: 155.96 - lr: 0.031250\n",
      "2021-04-28 09:13:40,558 epoch 15 - iter 4/10 - loss 49.40789318 - samples/sec: 143.01 - lr: 0.031250\n",
      "2021-04-28 09:13:47,557 epoch 15 - iter 5/10 - loss 49.08447495 - samples/sec: 146.36 - lr: 0.031250\n",
      "2021-04-28 09:13:54,575 epoch 15 - iter 6/10 - loss 46.33799680 - samples/sec: 145.98 - lr: 0.031250\n",
      "2021-04-28 09:14:01,127 epoch 15 - iter 7/10 - loss 43.68470383 - samples/sec: 156.33 - lr: 0.031250\n",
      "2021-04-28 09:14:08,252 epoch 15 - iter 8/10 - loss 42.46603346 - samples/sec: 143.77 - lr: 0.031250\n",
      "2021-04-28 09:14:15,131 epoch 15 - iter 9/10 - loss 41.70258628 - samples/sec: 148.89 - lr: 0.031250\n",
      "2021-04-28 09:14:15,479 epoch 15 - iter 10/10 - loss 41.69684334 - samples/sec: 2964.43 - lr: 0.031250\n",
      "2021-04-28 09:14:15,480 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:14:15,480 EPOCH 15 done: loss 41.6968 - lr 0.0312500\n",
      "2021-04-28 09:14:57,295 DEV : loss 41.230228424072266 - score 0.1037\n",
      "2021-04-28 09:15:07,856 TEST : loss 40.14235305786133 - score 0.1074\n",
      "Epoch    15: reducing learning rate of group 0 to 1.5625e-02.\n",
      "2021-04-28 09:15:08,079 BAD EPOCHS (no improvement): 2\n",
      "2021-04-28 09:15:08,083 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:15:08,084 resetting to best model\n",
      "2021-04-28 09:15:08,084 loading file resources/taggers/soft_ner/best-model.pt\n",
      "2021-04-28 09:15:23,513 epoch 16 - iter 1/5 - loss 57.71344376 - samples/sec: 153.37 - lr: 0.015625\n",
      "2021-04-28 09:15:37,549 epoch 16 - iter 2/5 - loss 48.45300293 - samples/sec: 145.95 - lr: 0.015625\n",
      "2021-04-28 09:15:51,622 epoch 16 - iter 3/5 - loss 43.65115611 - samples/sec: 145.56 - lr: 0.015625\n",
      "2021-04-28 09:16:06,254 epoch 16 - iter 4/5 - loss 41.22295475 - samples/sec: 140.00 - lr: 0.015625\n",
      "2021-04-28 09:16:13,742 epoch 16 - iter 5/5 - loss 40.07004166 - samples/sec: 273.59 - lr: 0.015625\n",
      "2021-04-28 09:16:13,744 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:16:13,745 EPOCH 16 done: loss 40.0700 - lr 0.0156250\n",
      "2021-04-28 09:16:56,184 DEV : loss 33.8927116394043 - score 0.2395\n",
      "2021-04-28 09:17:06,488 TEST : loss 34.33951950073242 - score 0.2259\n",
      "2021-04-28 09:17:06,704 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2021-04-28 09:17:10,274 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:17:24,097 epoch 17 - iter 1/5 - loss 36.73939896 - samples/sec: 148.20 - lr: 0.015625\n",
      "2021-04-28 09:17:37,989 epoch 17 - iter 2/5 - loss 29.35209942 - samples/sec: 147.46 - lr: 0.015625\n",
      "2021-04-28 09:17:52,031 epoch 17 - iter 3/5 - loss 27.85828845 - samples/sec: 145.88 - lr: 0.015625\n",
      "2021-04-28 09:18:06,152 epoch 17 - iter 4/5 - loss 27.85731363 - samples/sec: 145.07 - lr: 0.015625\n",
      "2021-04-28 09:18:13,833 epoch 17 - iter 5/5 - loss 30.39431038 - samples/sec: 266.69 - lr: 0.015625\n",
      "2021-04-28 09:18:13,834 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:18:13,835 EPOCH 17 done: loss 30.3943 - lr 0.0156250\n",
      "2021-04-28 09:18:56,080 DEV : loss 28.830766677856445 - score 0.2079\n",
      "2021-04-28 09:19:06,518 TEST : loss 29.259571075439453 - score 0.1998\n",
      "2021-04-28 09:19:06,734 BAD EPOCHS (no improvement): 1\n",
      "2021-04-28 09:19:06,738 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:19:20,372 epoch 18 - iter 1/5 - loss 47.21463776 - samples/sec: 150.25 - lr: 0.015625\n",
      "2021-04-28 09:19:33,962 epoch 18 - iter 2/5 - loss 39.96717072 - samples/sec: 150.73 - lr: 0.015625\n",
      "2021-04-28 09:19:48,385 epoch 18 - iter 3/5 - loss 32.98521614 - samples/sec: 142.02 - lr: 0.015625\n",
      "2021-04-28 09:20:02,501 epoch 18 - iter 4/5 - loss 30.43965673 - samples/sec: 145.12 - lr: 0.015625\n",
      "2021-04-28 09:20:09,920 epoch 18 - iter 5/5 - loss 29.55851974 - samples/sec: 276.12 - lr: 0.015625\n",
      "2021-04-28 09:20:09,921 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:20:09,923 EPOCH 18 done: loss 29.5585 - lr 0.0156250\n",
      "2021-04-28 09:20:52,068 DEV : loss 24.45252799987793 - score 0.1672\n",
      "2021-04-28 09:21:02,595 TEST : loss 26.527639389038086 - score 0.1612\n",
      "Epoch    18: reducing learning rate of group 0 to 7.8125e-03.\n",
      "2021-04-28 09:21:02,827 BAD EPOCHS (no improvement): 2\n",
      "2021-04-28 09:21:02,831 ----------------------------------------------------------------------------------------------------\n",
      "2021-04-28 09:21:02,832 resetting to best model\n",
      "2021-04-28 09:21:02,832 loading file resources/taggers/soft_ner/best-model.pt\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from flair.training_utils import AnnealOnPlateau\n",
    "# 6. start training\n",
    "trainer.train('resources/taggers/soft_ner',\n",
    "            learning_rate=0.5,\n",
    "            mini_batch_size=64,\n",
    "            max_epochs=20,\n",
    "            monitor_train=True,\n",
    "            monitor_test=True,\n",
    "            weight_decay=0.01,\n",
    "            scheduler=AnnealOnPlateau,\n",
    "            patience=1,\n",
    "            anneal_with_restarts=True,\n",
    "            batch_growth_annealing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-census",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
